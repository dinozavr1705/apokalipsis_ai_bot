import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, \
    classification_report, roc_curve, precision_recall_curve, auc
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import json
import time
import warnings

warnings.filterwarnings('ignore')

np.random.seed(42)


def generate_data(n_samples=20000):
    print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {n_samples} –∑–∞–ø–∏—Å–µ–π...")

    age = np.clip(np.random.normal(45, 20, n_samples), 0, 100).astype(int)
    physical = np.random.normal(7, 2, n_samples)
    physical = np.clip(physical, 1, 10).astype(int)
    iq = np.clip(np.random.normal(105, 20, n_samples), 40, 160).astype(int)
    vision = np.clip(100 - (age / 3) + np.random.normal(0, 15, n_samples), 0, 100).astype(int)
    parents = np.random.choice([0, 1, 2], n_samples, p=[0.1, 0.2, 0.7])
    movies = np.random.poisson(8, n_samples) + np.random.randint(0, 15, n_samples)
    movies = np.clip(movies, 0, 50).astype(int)
    autism = np.random.choice([0, 1], n_samples, p=[0.92, 0.08])

    age_factor = np.exp(-age / 40)
    physical_factor = physical / 10
    iq_factor = np.where(iq > 115, 0.9, np.where(iq > 100, 0.7, np.where(iq > 85, 0.5, 0.3)))
    vision_factor = vision / 100
    social_factor = parents / 2
    prep_factor = np.log1p(movies) / np.log1p(50)
    autism_penalty = np.where(autism == 1, -0.25, 0.0)

    young_strong = ((100 - age) / 100) * (physical / 10) * 0.3
    smart_prepared = (iq > 110).astype(float) * np.minimum(movies / 15, 1.0) * 0.25
    old_alone = (age > 70).astype(float) * (parents == 0).astype(float) * -0.35
    blind_old = (age > 60).astype(float) * ((100 - vision) / 100) * -0.2

    base_score = (
            age_factor * 0.25 +
            physical_factor * 0.20 +
            iq_factor * 0.15 +
            vision_factor * 0.10 +
            social_factor * 0.10 +
            prep_factor * 0.08 +
            autism_penalty
    )

    total_score = base_score + young_strong + smart_prepared + old_alone + blind_old
    survival_prob = 1 / (1 + np.exp(-12 * (total_score - 0.5)))

    uncertainty = np.abs(survival_prob - 0.5) / 0.5
    noise = np.random.normal(0, 0.08, n_samples) * uncertainty
    survival_prob += noise
    survival_prob = np.clip(survival_prob, 0.01, 0.99)

    survived = (survival_prob > 0.5).astype(int)

    df = pd.DataFrame({
        'age': age,
        'physical_ability': physical,
        'autism': autism,
        'parents_count': parents,
        'iq': iq,
        'vision': vision,
        'apocalypse_movies': movies,
        'survival_probability': survival_prob,
        'survived': survived
    })

    n_survived = df['survived'].sum()
    n_died = len(df) - n_survived
    n_target = min(n_survived, n_died)

    survived_indices = df[df['survived'] == 1].index[:n_target]
    died_indices = df[df['survived'] == 0].index[:n_target]
    balanced_indices = np.concatenate([survived_indices, died_indices])
    df = df.loc[balanced_indices].sample(frac=1, random_state=42).reset_index(drop=True)

    print(f"–î–∞–Ω–Ω—ã–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"–ë–∞–ª–∞–Ω—Å: {df['survived'].mean():.2%} –≤—ã–∂–∏–≤—à–∏—Ö")

    return df


class NeuralNetwork:
    def __init__(self, input_size, hidden_sizes=[64, 32], output_size=1, dropout_rate=0.2):
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.dropout_rate = dropout_rate
        self.training = True

        self.weights = []
        self.biases = []

        layer_sizes = [input_size] + hidden_sizes + [output_size]

        for i in range(len(layer_sizes) - 1):
            limit = np.sqrt(2.0 / layer_sizes[i])
            W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * limit
            b = np.zeros((1, layer_sizes[i + 1]))
            self.weights.append(W)
            self.biases.append(b)

        self.m_w = [np.zeros_like(w) for w in self.weights]
        self.v_w = [np.zeros_like(w) for w in self.weights]
        self.m_b = [np.zeros_like(b) for b in self.biases]
        self.v_b = [np.zeros_like(b) for b in self.biases]
        self.t = 0

        # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.gradient_norms = []
        self.weight_norms = []
        self.activation_stats = []

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return np.where(x > 0, 1, 0)

    def sigmoid(self, x):
        return np.where(x >= 0,
                        1 / (1 + np.exp(-x)),
                        np.exp(x) / (1 + np.exp(x)))

    def dropout(self, x, rate):
        if not self.training or rate == 0:
            return x
        mask = np.random.binomial(1, 1 - rate, size=x.shape) / (1 - rate)
        return x * mask

    def forward(self, X):
        self.activations = [X]
        self.z_values = []
        self.dropout_masks = []
        self.layer_outputs = []

        for i in range(len(self.hidden_sizes)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            a = self.relu(z)

            if i < len(self.hidden_sizes) - 1:
                a = self.dropout(a, self.dropout_rate)
                self.dropout_masks.append(a > 0 if self.training else None)

            self.activations.append(a)
            self.layer_outputs.append(a)

        z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]
        self.z_values.append(z_out)
        output = self.sigmoid(z_out)
        self.activations.append(output)
        self.layer_outputs.append(output)

        return output

    def backward(self, X, y, output, learning_rate):
        m = X.shape[0]
        dZ_out = output - y.reshape(-1, 1)

        gradients_w = []
        gradients_b = []

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        grad_norms = []

        dW_out = np.dot(self.activations[-2].T, dZ_out) / m
        db_out = np.sum(dZ_out, axis=0, keepdims=True) / m
        gradients_w.append(dW_out)
        gradients_b.append(db_out)
        grad_norms.append(np.linalg.norm(dW_out))

        dA = np.dot(dZ_out, self.weights[-1].T)

        for i in range(len(self.hidden_sizes) - 1, -1, -1):
            dZ = dA * self.relu_derivative(self.z_values[i])
            dW = np.dot(self.activations[i].T, dZ) / m
            db = np.sum(dZ, axis=0, keepdims=True) / m

            gradients_w.insert(0, dW)
            gradients_b.insert(0, db)
            grad_norms.insert(0, np.linalg.norm(dW))

            if i > 0:
                dA = np.dot(dZ, self.weights[i].T)

        self.gradient_norms.append(grad_norms)

        self.t += 1
        beta1, beta2 = 0.9, 0.999
        epsilon = 1e-8

        for i in range(len(self.weights)):
            self.m_w[i] = beta1 * self.m_w[i] + (1 - beta1) * gradients_w[i]
            self.v_w[i] = beta2 * self.v_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
            m_w_hat = self.m_w[i] / (1 - beta1 ** self.t)
            v_w_hat = self.v_w[i] / (1 - beta2 ** self.t)

            self.m_b[i] = beta1 * self.m_b[i] + (1 - beta1) * gradients_b[i]
            self.v_b[i] = beta2 * self.v_b[i] + (1 - beta2) * (gradients_b[i] ** 2)
            m_b_hat = self.m_b[i] / (1 - beta1 ** self.t)
            v_b_hat = self.v_b[i] / (1 - beta2 ** self.t)

            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º—ã –≤–µ—Å–æ–≤
        weight_norms = [np.linalg.norm(w) for w in self.weights]
        self.weight_norms.append(weight_norms)

    def compute_loss(self, y_true, y_pred):
        m = y_true.shape[0]
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        return cross_entropy

    def train(self, X, y, X_val, y_val, epochs=300, learning_rate=0.001, batch_size=32):
        self.training = True
        n_samples = X.shape[0]
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []

        # –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        train_precisions = []
        train_recalls = []
        train_f1_scores = []
        val_precisions = []
        val_recalls = []
        val_f1_scores = []

        best_val_acc = 0
        best_weights = None
        best_biases = None
        patience_counter = 0
        patience_limit = 20

        l2_lambda = 0.001
        start_time = time.time()

        for epoch in range(epochs):
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            epoch_loss = 0
            correct = 0
            n_batches = 0

            # –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ precision, recall, f1
            all_preds = []
            all_labels = []

            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                output = self.forward(X_batch)
                batch_loss = self.compute_loss(y_batch, output)

                l2_penalty = 0
                for w in self.weights:
                    l2_penalty += np.sum(w ** 2)
                batch_loss += l2_lambda * l2_penalty / (2 * batch_size)

                epoch_loss += batch_loss * len(X_batch)
                n_batches += 1

                preds = (output > 0.5).astype(int)
                correct += np.sum(preds.flatten() == y_batch)
                all_preds.extend(preds.flatten())
                all_labels.extend(y_batch)

                current_lr = learning_rate
                if epoch > 100:
                    current_lr = learning_rate * 0.5
                if epoch > 200:
                    current_lr = learning_rate * 0.1

                self.backward(X_batch, y_batch, output, current_lr)

            avg_loss = epoch_loss / n_samples
            train_acc = correct / n_samples

            # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            from sklearn.metrics import precision_score, recall_score, f1_score
            train_precision = precision_score(all_labels, all_preds, zero_division=0)
            train_recall = recall_score(all_labels, all_preds, zero_division=0)
            train_f1 = f1_score(all_labels, all_preds, zero_division=0)

            train_losses.append(avg_loss)
            train_accuracies.append(train_acc)
            train_precisions.append(train_precision)
            train_recalls.append(train_recall)
            train_f1_scores.append(train_f1)

            self.training = False
            val_output = self.forward(X_val)
            self.training = True

            val_loss = self.compute_loss(y_val, val_output)
            val_preds = (val_output > 0.5).astype(int)
            val_acc = np.mean(val_preds.flatten() == y_val)

            val_precision = precision_score(y_val, val_preds.flatten(), zero_division=0)
            val_recall = recall_score(y_val, val_preds.flatten(), zero_division=0)
            val_f1 = f1_score(y_val, val_preds.flatten(), zero_division=0)

            val_losses.append(val_loss)
            val_accuracies.append(val_acc)
            val_precisions.append(val_precision)
            val_recalls.append(val_recall)
            val_f1_scores.append(val_f1)

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience_limit:
                print(f"\n–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}: –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π –≤ —Ç–µ—á–µ–Ω–∏–µ {patience_limit} —ç–ø–æ—Ö")
                break

            if (epoch + 1) % 30 == 0 or epoch == 0 or epoch == epochs - 1:
                elapsed = time.time() - start_time
                progress = (epoch + 1) / epochs * 100
                print(f"–≠–ø–æ—Ö–∞ {epoch + 1:3d}/{epochs} ({progress:5.1f}%) | "
                      f"Loss: {avg_loss:.4f}‚Üí{val_loss:.4f} | "
                      f"Acc: {train_acc:.4f}‚Üí{val_acc:.4f} | "
                      f"F1: {train_f1:.4f}‚Üí{val_f1:.4f} | "
                      f"–õ—É—á—à–∞—è val: {best_val_acc:.4f}")

        if best_weights is not None:
            self.weights = best_weights
            self.biases = best_biases

        self.training = False

        total_time = time.time() - start_time
        print(f"\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time / 60:.1f} –º–∏–Ω—É—Ç")
        print(f"–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π train accuracy: {train_accuracies[-1]:.4f}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π val accuracy: {val_accuracies[-1]:.4f}")
        print(f"Loss —É–º–µ–Ω—å—à–∏–ª—Å—è: {train_losses[0]:.4f} ‚Üí {train_losses[-1]:.4f}")

        gap = train_accuracies[-1] - val_accuracies[-1]
        if gap > 0.1:
            print(f"‚ö†Ô∏è  –ë–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ train-val: {gap:.4f} (–≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)")
        elif gap > 0.05:
            print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ train-val: {gap:.4f}")
        else:
            print(f"‚úÖ  –•–æ—Ä–æ—à–∏–π —Ä–∞–∑—Ä—ã–≤ train-val: {gap:.4f}")

        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'train_precisions': train_precisions,
            'train_recalls': train_recalls,
            'train_f1_scores': train_f1_scores,
            'val_precisions': val_precisions,
            'val_recalls': val_recalls,
            'val_f1_scores': val_f1_scores,
            'gradient_norms': self.gradient_norms,
            'weight_norms': self.weight_norms
        }

    def predict_proba(self, X):
        self.training = False
        return self.forward(X)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) > threshold).astype(int)

    def evaluate(self, X, y):
        y_pred_proba = self.predict_proba(X).flatten()
        y_pred = (y_pred_proba > 0.5).astype(int)

        return {
            'accuracy': accuracy_score(y, y_pred),
            'precision': precision_score(y, y_pred, zero_division=0),
            'recall': recall_score(y, y_pred, zero_division=0),
            'f1_score': f1_score(y, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y, y_pred_proba),
            'confusion_matrix': confusion_matrix(y, y_pred),
            'y_pred_proba': y_pred_proba,
            'y_true': y
        }


def plot_advanced_visualizations(df, nn, results, train_results, X_test_scaled, y_test):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–∏"""

    # –§–∏–≥—É—Ä–∞ 1: –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    fig1, axes1 = plt.subplots(2, 3, figsize=(18, 12))

    # 1. Loss
    axes1[0, 0].plot(train_results['train_losses'], label='Train', linewidth=2, alpha=0.8)
    axes1[0, 0].plot(train_results['val_losses'], label='Val', linewidth=2, alpha=0.8)
    axes1[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes1[0, 0].set_ylabel('Loss')
    axes1[0, 0].set_title('Loss –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes1[0, 0].legend()
    axes1[0, 0].grid(True, alpha=0.3)

    # 2. Accuracy
    axes1[0, 1].plot(train_results['train_accuracies'], label='Train', linewidth=2, alpha=0.8)
    axes1[0, 1].plot(train_results['val_accuracies'], label='Val', linewidth=2, alpha=0.8)
    axes1[0, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='–¶–µ–ª—å 0.9')
    axes1[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes1[0, 1].set_ylabel('Accuracy')
    axes1[0, 1].set_title('Accuracy –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes1[0, 1].legend()
    axes1[0, 1].grid(True, alpha=0.3)

    # 3. F1 Score
    axes1[0, 2].plot(train_results['train_f1_scores'], label='Train F1', linewidth=2, alpha=0.8)
    axes1[0, 2].plot(train_results['val_f1_scores'], label='Val F1', linewidth=2, alpha=0.8)
    axes1[0, 2].set_xlabel('–≠–ø–æ—Ö–∞')
    axes1[0, 2].set_ylabel('F1 Score')
    axes1[0, 2].set_title('F1 Score –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes1[0, 2].legend()
    axes1[0, 2].grid(True, alpha=0.3)

    # 4. Confusion Matrix
    cm = results['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['–ù–µ –≤—ã–∂–∏–ª', '–í—ã–∂–∏–ª'],
                yticklabels=['–ù–µ –≤—ã–∂–∏–ª', '–í—ã–∂–∏–ª'],
                ax=axes1[1, 0])
    axes1[1, 0].set_title(f'Confusion Matrix (Accuracy: {results["accuracy"]:.2%})')
    axes1[1, 0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    axes1[1, 0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

    # 5. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    y_test_proba = results['y_pred_proba']
    axes1[1, 1].hist(y_test_proba, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes1[1, 1].axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='–ü–æ—Ä–æ–≥ 0.5')
    axes1[1, 1].set_xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–∂–∏–≤–∞–Ω–∏—è')
    axes1[1, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤')
    axes1[1, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
    axes1[1, 1].legend()
    axes1[1, 1].grid(True, alpha=0.3)

    # 6. Precision-Recall
    axes1[1, 2].plot(train_results['train_recalls'], train_results['train_precisions'],
                     label='Train', linewidth=2, alpha=0.7)
    axes1[1, 2].plot(train_results['val_recalls'], train_results['val_precisions'],
                     label='Val', linewidth=2, alpha=0.7)
    axes1[1, 2].set_xlabel('Recall')
    axes1[1, 2].set_ylabel('Precision')
    axes1[1, 2].set_title('Precision-Recall –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    axes1[1, 2].legend()
    axes1[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_metrics_comprehensive.png', dpi=100, bbox_inches='tight')
    plt.show()

    # –§–∏–≥—É—Ä–∞ 2: ROC –∏ Precision-Recall –∫—Ä–∏–≤—ã–µ
    fig2, axes2 = plt.subplots(1, 3, figsize=(18, 6))

    # ROC –∫—Ä–∏–≤–∞—è
    fpr, tpr, thresholds = roc_curve(results['y_true'], results['y_pred_proba'])
    roc_auc = auc(fpr, tpr)

    axes2[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC –∫—Ä–∏–≤–∞—è (AUC = {roc_auc:.4f})')
    axes2[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes2[0].set_xlim([0.0, 1.0])
    axes2[0].set_ylim([0.0, 1.05])
    axes2[0].set_xlabel('False Positive Rate')
    axes2[0].set_ylabel('True Positive Rate')
    axes2[0].set_title('ROC –∫—Ä–∏–≤–∞—è')
    axes2[0].legend(loc="lower right")
    axes2[0].grid(True, alpha=0.3)

    # Precision-Recall –∫—Ä–∏–≤–∞—è
    precision, recall, thresholds = precision_recall_curve(results['y_true'], results['y_pred_proba'])
    pr_auc = auc(recall, precision)

    axes2[1].plot(recall, precision, color='green', lw=2, label=f'PR –∫—Ä–∏–≤–∞—è (AUC = {pr_auc:.4f})')
    axes2[1].set_xlim([0.0, 1.0])
    axes2[1].set_ylim([0.0, 1.05])
    axes2[1].set_xlabel('Recall')
    axes2[1].set_ylabel('Precision')
    axes2[1].set_title('Precision-Recall –∫—Ä–∏–≤–∞—è')
    axes2[1].legend(loc="lower left")
    axes2[1].grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º
    errors = np.abs(results['y_pred_proba'] - results['y_true'])
    axes2[2].scatter(results['y_pred_proba'], errors, alpha=0.5, s=10)
    axes2[2].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes2[2].set_ylabel('–ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
    axes2[2].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫')
    axes2[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('roc_pr_curves.png', dpi=100, bbox_inches='tight')
    plt.show()

    # –§–∏–≥—É—Ä–∞ 3: –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –≤–µ—Å–æ–≤
    if hasattr(nn, 'gradient_norms') and nn.gradient_norms:
        fig3, axes3 = plt.subplots(2, 2, figsize=(15, 12))

        # –ù–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–ª–æ—è–º
        gradient_norms = np.array(nn.gradient_norms)
        for i in range(gradient_norms.shape[1]):
            axes3[0, 0].plot(gradient_norms[:, i], label=f'–°–ª–æ–π {i + 1}', alpha=0.7)
        axes3[0, 0].set_xlabel('–≠–ø–æ—Ö–∞ (–º–∏–Ω–∏-–±–∞—Ç—á–∏)')
        axes3[0, 0].set_ylabel('–ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞')
        axes3[0, 0].set_title('–ù–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–ª–æ—è–º')
        axes3[0, 0].legend()
        axes3[0, 0].grid(True, alpha=0.3)

        # –ù–æ—Ä–º—ã –≤–µ—Å–æ–≤ –ø–æ —Å–ª–æ—è–º
        weight_norms = np.array(nn.weight_norms)
        for i in range(weight_norms.shape[1]):
            axes3[0, 1].plot(weight_norms[:, i], label=f'–°–ª–æ–π {i + 1}', alpha=0.7)
        axes3[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes3[0, 1].set_ylabel('–ù–æ—Ä–º–∞ –≤–µ—Å–æ–≤')
        axes3[0, 1].set_title('–ù–æ—Ä–º—ã –≤–µ—Å–æ–≤ –ø–æ —Å–ª–æ—è–º')
        axes3[0, 1].legend()
        axes3[0, 1].grid(True, alpha=0.3)

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ
        if len(nn.weights) > 0:
            weights_first = nn.weights[0].flatten()
            axes3[1, 0].hist(weights_first, bins=50, alpha=0.7, color='purple', edgecolor='black')
            axes3[1, 0].set_xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ –≤–µ—Å–∞')
            axes3[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            axes3[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ')
            axes3[1, 0].grid(True, alpha=0.3)

            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–µ—Å–æ–≤)
            feature_importance = np.abs(nn.weights[0]).mean(axis=1)
            features = ['age', 'physical', 'autism', 'parents', 'iq', 'vision', 'movies']
            axes3[1, 1].barh(features, feature_importance, color='teal')
            axes3[1, 1].set_xlabel('–°—Ä–µ–¥–Ω–µ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–µ—Å–∞')
            axes3[1, 1].set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–µ—Ä–≤—ã–π —Å–ª–æ–π)')
            axes3[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('gradients_weights_analysis.png', dpi=100, bbox_inches='tight')
        plt.show()

    # –§–∏–≥—É—Ä–∞ 4: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    fig4, axes4 = plt.subplots(3, 3, figsize=(18, 15))

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞
    axes4[0, 0].hist(df[df['survived'] == 0]['age'], alpha=0.5, label='–ù–µ –≤—ã–∂–∏–ª', bins=20)
    axes4[0, 0].hist(df[df['survived'] == 1]['age'], alpha=0.5, label='–í—ã–∂–∏–ª', bins=20)
    axes4[0, 0].set_xlabel('–í–æ–∑—Ä–∞—Å—Ç')
    axes4[0, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes4[0, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞')
    axes4[0, 0].legend()
    axes4[0, 0].grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
    axes4[0, 1].hist(df[df['survived'] == 0]['physical_ability'], alpha=0.5, label='–ù–µ –≤—ã–∂–∏–ª', bins=10)
    axes4[0, 1].hist(df[df['survived'] == 1]['physical_ability'], alpha=0.5, label='–í—ã–∂–∏–ª', bins=10)
    axes4[0, 1].set_xlabel('–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏')
    axes4[0, 1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes4[0, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π')
    axes4[0, 1].legend()
    axes4[0, 1].grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ IQ
    axes4[0, 2].hist(df[df['survived'] == 0]['iq'], alpha=0.5, label='–ù–µ –≤—ã–∂–∏–ª', bins=20)
    axes4[0, 2].hist(df[df['survived'] == 1]['iq'], alpha=0.5, label='–í—ã–∂–∏–ª', bins=20)
    axes4[0, 2].set_xlabel('IQ')
    axes4[0, 2].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes4[0, 2].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ IQ')
    axes4[0, 2].legend()
    axes4[0, 2].grid(True, alpha=0.3)

    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    corr_matrix = df.corr()
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',
                center=0, ax=axes4[1, 0])
    axes4[1, 0].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')

    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤—ã–∂–∏–≤–∞–Ω–∏—è –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã
    scatter = axes4[1, 1].scatter(df['age'], df['physical_ability'],
                                  c=df['survived'], cmap='RdYlGn', alpha=0.6)
    axes4[1, 1].set_xlabel('–í–æ–∑—Ä–∞—Å—Ç')
    axes4[1, 1].set_ylabel('–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏')
    axes4[1, 1].set_title('–í—ã–∂–∏–≤–∞–Ω–∏–µ: –≤–æ–∑—Ä–∞—Å—Ç vs —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞')
    axes4[1, 1].grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=axes4[1, 1], label='–í—ã–∂–∏–ª (1) / –ù–µ –≤—ã–∂–∏–ª (0)')

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–ª—å–º–æ–≤
    axes4[1, 2].hist(df[df['survived'] == 0]['apocalypse_movies'], alpha=0.5, label='–ù–µ –≤—ã–∂–∏–ª', bins=15)
    axes4[1, 2].hist(df[df['survived'] == 1]['apocalypse_movies'], alpha=0.5, label='–í—ã–∂–∏–ª', bins=15)
    axes4[1, 2].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
    axes4[1, 2].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes4[1, 2].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤')
    axes4[1, 2].legend()
    axes4[1, 2].grid(True, alpha=0.3)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–µ–Ω–∏–µ vs IQ
    scatter = axes4[2, 0].scatter(df['vision'], df['iq'],
                                  c=df['survived'], cmap='RdYlGn', alpha=0.6)
    axes4[2, 0].set_xlabel('–ó—Ä–µ–Ω–∏–µ (%)')
    axes4[2, 0].set_ylabel('IQ')
    axes4[2, 0].set_title('–í—ã–∂–∏–≤–∞–Ω–∏–µ: –∑—Ä–µ–Ω–∏–µ vs IQ')
    axes4[2, 0].grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=axes4[2, 0], label='–í—ã–∂–∏–ª (1) / –ù–µ –≤—ã–∂–∏–ª (0)')

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã–∂–∏–≤–∞–Ω–∏—è –ø–æ —Ä–æ–¥–∏—Ç–µ–ª—è–º
    parent_survival = df.groupby('parents_count')['survived'].mean()
    axes4[2, 1].bar(parent_survival.index, parent_survival.values, color='orange')
    axes4[2, 1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∂–∏–≤—ã—Ö —Ä–æ–¥–∏—Ç–µ–ª–µ–π')
    axes4[2, 1].set_ylabel('–î–æ–ª—è –≤—ã–∂–∏–≤—à–∏—Ö')
    axes4[2, 1].set_title('–í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–æ–¥–∏—Ç–µ–ª–µ–π')
    axes4[2, 1].grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—É—Ç–∏–∑–º–∞
    autism_survival = df.groupby('autism')['survived'].mean()
    axes4[2, 2].bar(autism_survival.index, autism_survival.values, color='purple')
    axes4[2, 2].set_xlabel('–ê—É—Ç–∏–∑–º (0=–Ω–µ—Ç, 1=–¥–∞)')
    axes4[2, 2].set_ylabel('–î–æ–ª—è –≤—ã–∂–∏–≤—à–∏—Ö')
    axes4[2, 2].set_title('–í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å –ø—Ä–∏ –∞—É—Ç–∏–∑–º–µ')
    axes4[2, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('data_analysis_comprehensive.png', dpi=100, bbox_inches='tight')
    plt.show()

    # –§–∏–≥—É—Ä–∞ 5: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    fig5, axes5 = plt.subplots(2, 2, figsize=(15, 12))

    # –ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω–∞—è –∫—Ä–∏–≤–∞—è
    from sklearn.calibration import calibration_curve
    prob_true, prob_pred = calibration_curve(results['y_true'], results['y_pred_proba'], n_bins=10)

    axes5[0, 0].plot([0, 1], [0, 1], "k:", label="–ò–¥–µ–∞–ª—å–Ω–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–æ")
    axes5[0, 0].plot(prob_pred, prob_true, "s-", label="–ù–∞—à–∞ –º–æ–¥–µ–ª—å")
    axes5[0, 0].set_xlabel('–°—Ä–µ–¥–Ω—è—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes5[0, 0].set_ylabel('–î–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö')
    axes5[0, 0].set_title('–ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω–∞—è –∫—Ä–∏–≤–∞—è')
    axes5[0, 0].legend()
    axes5[0, 0].grid(True, alpha=0.3)

    # –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–æ–≤
    thresholds = np.linspace(0.1, 0.9, 9)
    accuracies = []
    f1_scores = []

    for threshold in thresholds:
        y_pred_threshold = (results['y_pred_proba'] > threshold).astype(int)
        accuracies.append(accuracy_score(results['y_true'], y_pred_threshold))
        f1_scores.append(f1_score(results['y_true'], y_pred_threshold, zero_division=0))

    axes5[0, 1].plot(thresholds, accuracies, 'o-', label='Accuracy')
    axes5[0, 1].plot(thresholds, f1_scores, 's-', label='F1 Score')
    axes5[0, 1].axvline(x=0.5, color='r', linestyle='--', alpha=0.5, label='–ü–æ—Ä–æ–≥ 0.5')
    axes5[0, 1].set_xlabel('–ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')
    axes5[0, 1].set_ylabel('–ú–µ—Ç—Ä–∏–∫–∞')
    axes5[0, 1].set_title('–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –æ—Ç –ø–æ—Ä–æ–≥–∞')
    axes5[0, 1].legend()
    axes5[0, 1].grid(True, alpha=0.3)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
    y_pred = (results['y_pred_proba'] > 0.5).astype(int)
    errors = y_pred != results['y_true']

    axes5[1, 0].hist(results['y_pred_proba'][errors], bins=20, alpha=0.7, color='red',
                     label='–û—à–∏–±–∫–∏', edgecolor='black')
    axes5[1, 0].hist(results['y_pred_proba'][~errors], bins=20, alpha=0.7, color='green',
                     label='–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ', edgecolor='black')
    axes5[1, 0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes5[1, 0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    axes5[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ—à–∏–±–æ–∫ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
    axes5[1, 0].legend()
    axes5[1, 0].grid(True, alpha=0.3)

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Reds',
                xticklabels=['–ù–µ –≤—ã–∂–∏–ª', '–í—ã–∂–∏–ª'],
                yticklabels=['–ù–µ –≤—ã–∂–∏–ª', '–í—ã–∂–∏–ª'],
                ax=axes5[1, 1])
    axes5[1, 1].set_title('Confusion Matrix (%)')
    axes5[1, 1].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    axes5[1, 1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')

    plt.tight_layout()
    plt.savefig('predictions_detailed_analysis.png', dpi=100, bbox_inches='tight')
    plt.show()


def main():
    print("=" * 80)
    print("–†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –í–´–ñ–ò–í–ê–ù–ò–Ø")
    print("–° –ö–û–ú–ü–õ–ï–ö–°–ù–û–ô –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ï–ô")
    print("=" * 80)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    df = generate_data(20000)

    features = ['age', 'physical_ability', 'autism', 'parents_count',
                'iq', 'vision', 'apocalypse_movies']
    target = 'survived'

    X = df[features].values
    y = df[target].values

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    input_size = X_train.shape[1]
    nn = NeuralNetwork(input_size=input_size, hidden_sizes=[32, 16], dropout_rate=0.3)

    print(f"\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏: {input_size} ‚Üí [32, 16] ‚Üí 1 (Dropout: 0.3)")
    print(f"–î–∞–Ω–Ω—ã–µ: train={X_train.shape[0]}, val={X_val.shape[0]}, test={X_test.shape[0]}")
    print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y_train.mean():.2%} –≤—ã–∂–∏–≤—à–∏—Ö")

    print(f"\n–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    train_results = nn.train(
        X_train_scaled, y_train,
        X_val_scaled, y_val,
        epochs=200,
        learning_rate=0.0008,
        batch_size=64
    )

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    results = nn.evaluate(X_test_scaled, y_test)

    print(f"\n" + "=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–ï:")
    print("=" * 80)
    print(f"Accuracy:  {results['accuracy']:.4f}")
    print(f"Precision: {results['precision']:.4f}")
    print(f"Recall:    {results['recall']:.4f}")
    print(f"F1-score:  {results['f1_score']:.4f}")
    print(f"ROC-AUC:   {results['roc_auc']:.4f}")

    print(f"\n" + "=" * 80)
    print("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–ï–õ–ò:")
    print("=" * 80)

    y_test_proba = results['y_pred_proba']
    confidence_high = np.mean((y_test_proba > 0.8) | (y_test_proba < 0.2))
    confidence_medium = np.mean(((y_test_proba > 0.7) & (y_test_proba <= 0.8)) |
                                ((y_test_proba >= 0.2) & (y_test_proba < 0.3)))
    confidence_low = np.mean((y_test_proba >= 0.3) & (y_test_proba <= 0.7))

    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print(f"  –í—ã—Å–æ–∫–∞—è (>0.8 –∏–ª–∏ <0.2): {confidence_high:.1%}")
    print(f"  –°—Ä–µ–¥–Ω—è—è (0.7-0.8 –∏–ª–∏ 0.2-0.3): {confidence_medium:.1%}")
    print(f"  –ù–∏–∑–∫–∞—è (0.3-0.7): {confidence_low:.1%}")

    if results['accuracy'] >= 0.9:
        print("\nüéØ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! Accuracy > 0.9")
    elif results['accuracy'] >= 0.85:
        print("\n‚ö†Ô∏è  –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –º–æ–∂–Ω–æ –ª—É—á—à–µ")
    else:
        print("\nüî¥ –ù—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –º–æ–¥–µ–ª—å")

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\n" + "=" * 80)
    print("–°–û–ó–î–ê–ù–ò–ï –ö–û–ú–ü–õ–ï–ö–°–ù–´–• –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô...")
    print("=" * 80)

    plot_advanced_visualizations(df, nn, results, train_results, X_test_scaled, y_test)

    # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    test_cases = [
        [25, 9, 0, 2, 130, 100, 25],
        [80, 3, 0, 0, 90, 60, 5],
        [35, 8, 0, 2, 115, 90, 15],
        [95, 2, 0, 1, 85, 30, 1],
        [45, 7, 0, 2, 110, 80, 30],
        [30, 6, 1, 1, 120, 85, 10],
    ]

    test_cases_scaled = scaler.transform(test_cases)
    predictions = nn.predict_proba(test_cases_scaled)

    print(f"\n" + "=" * 80)
    print("–ü–†–ò–ú–ï–†–´ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
    print("=" * 80)

    for i, (case, pred) in enumerate(zip(test_cases, predictions), 1):
        prob = pred[0]
        if prob > 0.8 or prob < 0.2:
            confidence = "–æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ"
        elif prob > 0.7 or prob < 0.3:
            confidence = "—É–≤–µ—Ä–µ–Ω–Ω–æ"
        else:
            confidence = "–Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ"
        binary = "–í–´–ñ–ò–í–ï–¢" if prob > 0.5 else "–ù–ï –í–´–ñ–ò–í–ï–¢"
        print(f"\n{i}. –í–æ–∑—Ä–∞—Å—Ç={case[0]}, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏={case[1]}/10, "
              f"–∞—É—Ç–∏–∑–º={'–¥–∞' if case[2] == 1 else '–Ω–µ—Ç'}, —Ä–æ–¥–∏—Ç–µ–ª–∏={case[3]}, "
              f"iq={case[4]}, –∑—Ä–µ–Ω–∏–µ={case[5]}%, —Ñ–∏–ª—å–º—ã={case[6]}")
        print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prob:.1%} ({confidence})")
        print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {binary}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_data = {
        'weights_0': nn.weights[0],
        'weights_1': nn.weights[1],
        'weights_2': nn.weights[2],
        'biases_0': nn.biases[0],
        'biases_1': nn.biases[1],
        'biases_2': nn.biases[2],
        'scaler_mean': scaler.mean_,
        'scaler_scale': scaler.scale_,
        'features': features
    }

    np.savez('final_model_advanced.npz', **model_data)
    print(f"\n–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'final_model_advanced.npz'")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    model_info = {
        'accuracy': float(results['accuracy']),
        'precision': float(results['precision']),
        'recall': float(results['recall']),
        'f1_score': float(results['f1_score']),
        'roc_auc': float(results['roc_auc']),
        'architecture': [input_size, 32, 16, 1],
        'features': features,
        'epochs_trained': len(train_results['train_losses']),
        'best_val_accuracy': float(max(train_results['val_accuracies'])),
        'final_val_accuracy': float(train_results['val_accuracies'][-1]),
        'final_train_loss': float(train_results['train_losses'][-1]),
        'final_val_loss': float(train_results['val_losses'][-1]),
        'train_val_gap': float(train_results['train_accuracies'][-1] - train_results['val_accuracies'][-1]),
        'dropout_rate': 0.3,
        'learning_rate': 0.0008,
        'batch_size': 64,
        'confidence_distribution': {
            'high': float(confidence_high),
            'medium': float(confidence_medium),
            'low': float(confidence_low)
        },
        'notes': '–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π'
    }

    with open('model_info_advanced.json', 'w') as f:
        json.dump(model_info, f, indent=2)

    print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'model_info_advanced.json'")

    print(f"\n" + "=" * 80)
    print("–°–û–ó–î–ê–ù–û 5 –ö–û–ú–ü–õ–ï–ö–°–ù–´–• –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô:")
    print("1. training_metrics_comprehensive.png - –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
    print("2. roc_pr_curves.png - ROC –∏ Precision-Recall –∫—Ä–∏–≤—ã–µ")
    print("3. gradients_weights_analysis.png - –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –≤–µ—Å–æ–≤")
    print("4. data_analysis_comprehensive.png - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    print("5. predictions_detailed_analysis.png - –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    print("=" * 80)


if __name__ == "__main__":
    main()